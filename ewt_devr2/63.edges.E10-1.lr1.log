2019-01-18 14:40:21,718 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/63.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/63.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 14:40:21,719 No Model Save selected!
2019-01-18 14:40:21,720 Loaded Graph data/edges/parsing/ewt/ewt_dev/63.edges with 17 nodes scale=1.0
2019-01-18 14:40:21,720 Building dataset
2019-01-18 14:40:21,720 Subsample: 17 points with scale 1.0 subsample=16
2019-01-18 14:40:21,739 Built Data Sampler
2019-01-18 14:40:21,739 Creating a fresh model warm_start?=None
2019-01-18 14:40:21,739 	 Warmstarting? None None 17
2019-01-18 14:40:21,739 Embedding() torch.Size([17, 10])
2019-01-18 14:40:21,739 relative No Rescale
2019-01-18 14:40:21,740 Constructed model with dim=0 and epochs=0 isnan=False
2019-01-18 14:40:21,740 SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 1
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 2
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 3
    dampening: 0
    initial_lr: 0.0001
    lr: 0.001
    momentum: 0.0
    nesterov: False
    weight_decay: 0
)
2019-01-18 14:40:21,740 *** Initial Checkpoint. Computing Stats
2019-01-18 14:40:21,740 	 Computing Major Stats lazily... 
2019-01-18 14:40:21,742 		 Completed edges=272 good=272 bad=0
2019-01-18 14:40:21,744 Distortion avg=0.9982445404247537 wc=11.75916903674143 me=0.005982886981094694 mc=1965.4673527845657 nan_elements=0
2019-01-18 14:40:21,745 MAP = 0.18295838148779328
2019-01-18 14:40:21,745 scale=[array([1.])]
2019-01-18 14:40:21,745 *** End Initial Checkpoint

2019-01-18 14:40:21,807 1 loss=0.9964934371264366
2019-01-18 14:40:21,808 2 loss=0.9119868750249986
2019-01-18 14:40:21,808 3 loss=0.8368097501384842
2019-01-18 14:40:21,809 4 loss=0.7722318323664691
2019-01-18 14:40:21,810 5 loss=0.7168589976299597
2019-01-18 14:40:21,811 6 loss=0.6692637974846488
2019-01-18 14:40:21,812 7 loss=0.6281912180531171
2019-01-18 14:40:21,813 8 loss=0.5925776883190943
2019-01-18 14:40:21,814 9 loss=0.5615342244606466
2019-01-18 14:40:21,815 10 loss=0.5343222884971905
2019-01-18 14:40:21,815 11 loss=0.5103300000694987
2019-01-18 14:40:21,816 12 loss=0.4890509076513871
2019-01-18 14:40:21,817 13 loss=0.47006579270164156
2019-01-18 14:40:21,817 14 loss=0.45302737954530925
2019-01-18 14:40:21,818 15 loss=0.43764762524718664
2019-01-18 14:40:21,819 16 loss=0.42368722013363624
2019-01-18 14:40:21,819 17 loss=0.4109469466441429
2019-01-18 14:40:21,820 18 loss=0.3992605830747835
2019-01-18 14:40:21,821 19 loss=0.38848908296085405
2019-01-18 14:40:21,821 20 loss=0.37851580332566
2019-01-18 14:40:21,822 21 loss=0.3692425930699522
2019-01-18 14:40:21,823 22 loss=0.36058658562230855
2019-01-18 14:40:21,823 23 loss=0.3524775677266458
2019-01-18 14:40:21,824 24 loss=0.344855819388614
2019-01-18 14:40:21,825 25 loss=0.33767033913849165
2019-01-18 14:40:21,825 26 loss=0.33087738449907655
2019-01-18 14:40:21,826 27 loss=0.3244392704296755
2019-01-18 14:40:21,827 28 loss=0.31832337904091995
2019-01-18 14:40:21,827 29 loss=0.31250134245802785
2019-01-18 14:40:21,828 30 loss=0.30694836770378425
2019-01-18 14:40:21,829 31 loss=0.30164267816878615
2019-01-18 14:40:21,830 32 loss=0.29656505087589846
2019-01-18 14:40:21,830 33 loss=0.29169843252544525
2019-01-18 14:40:21,831 34 loss=0.2870276203882491
2019-01-18 14:40:21,832 35 loss=0.28253899662615034
2019-01-18 14:40:21,833 36 loss=0.27822030667046377
2019-01-18 14:40:21,834 37 loss=0.2740604739643054
2019-01-18 14:40:21,835 38 loss=0.2700494447448718
2019-01-18 14:40:21,835 39 loss=0.2661780576633796
2019-01-18 14:40:21,836 40 loss=0.2624379339595285
2019-01-18 14:40:21,837 41 loss=0.2588213846614105
2019-01-18 14:40:21,837 42 loss=0.2553213319010813
2019-01-18 14:40:21,838 43 loss=0.2519312419451824
2019-01-18 14:40:21,839 44 loss=0.24864506795907879
2019-01-18 14:40:21,839 45 loss=0.24545720086825493
2019-01-18 14:40:21,840 46 loss=0.24236242696546395
2019-01-18 14:40:21,841 47 loss=0.23935589114716785
2019-01-18 14:40:21,841 48 loss=0.2364330648569677
2019-01-18 14:40:21,842 49 loss=0.2335897179742173
2019-01-18 14:40:21,843 50 loss=0.23082189401873351
2019-01-18 14:40:21,843 final loss=0.23082189401873351
2019-01-18 14:40:21,843 best loss=10000000000.0, distortion=10000000000.0, map=0.0, wc_dist=10000000000.0
2019-01-18 14:40:21,843 	 Computing Major Stats lazily... 
2019-01-18 14:40:21,844 		 Completed edges=272 good=272 bad=0
2019-01-18 14:40:21,847 Distortion avg=0.4254867797639289 wc=7.512825999521182 me=1.704384540304887 mc=4.407940709305693 nan_elements=0
2019-01-18 14:40:21,847 MAP = 0.40884803921568624
2019-01-18 14:40:21,847 scale=[array([1.])]
2019-01-18 15:16:54,098 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/63.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/63.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '--checkpoint-freq', '10', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 15:16:54,099 No Model Save selected!
