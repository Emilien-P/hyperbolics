2019-01-18 14:42:15,564 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/1300.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/1300.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 14:42:15,564 No Model Save selected!
2019-01-18 14:42:15,566 Loaded Graph data/edges/parsing/ewt/ewt_dev/1300.edges with 7 nodes scale=1.0
2019-01-18 14:42:15,566 Building dataset
2019-01-18 14:42:15,566 Subsample: 7 points with scale 1.0 subsample=6
2019-01-18 14:42:15,571 Built Data Sampler
2019-01-18 14:42:15,572 Creating a fresh model warm_start?=None
2019-01-18 14:42:15,572 	 Warmstarting? None None 7
2019-01-18 14:42:15,572 Embedding() torch.Size([7, 10])
2019-01-18 14:42:15,572 relative No Rescale
2019-01-18 14:42:15,572 Constructed model with dim=0 and epochs=0 isnan=False
2019-01-18 14:42:15,573 SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 1
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 2
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 3
    dampening: 0
    initial_lr: 0.0001
    lr: 0.001
    momentum: 0.0
    nesterov: False
    weight_decay: 0
)
2019-01-18 14:42:15,573 *** Initial Checkpoint. Computing Stats
2019-01-18 14:42:15,573 	 Computing Major Stats lazily... 
2019-01-18 14:42:15,574 		 Completed edges=42 good=42 bad=0
2019-01-18 14:42:15,575 Distortion avg=0.9972458731989694 wc=4.921346900398354 me=0.005939956548066182 mc=828.5156398998494 nan_elements=0
2019-01-18 14:42:15,575 MAP = 0.4119047619047619
2019-01-18 14:42:15,575 scale=[array([1.])]
2019-01-18 14:42:15,575 *** End Initial Checkpoint

2019-01-18 14:42:15,634 1 loss=0.9945012509553001
2019-01-18 14:42:15,634 2 loss=0.5783613991349839
2019-01-18 14:42:15,635 3 loss=0.40189540157764286
2019-01-18 14:42:15,635 4 loss=0.3183673957720869
2019-01-18 14:42:15,636 5 loss=0.26751069303711494
2019-01-18 14:42:15,636 6 loss=0.23074057226484493
2019-01-18 14:42:15,637 7 loss=0.20187456959457314
2019-01-18 14:42:15,637 8 loss=0.17839785044432574
2019-01-18 14:42:15,638 9 loss=0.15896523391538078
2019-01-18 14:42:15,638 10 loss=0.14267774462202484
2019-01-18 14:42:15,639 11 loss=0.12886488083136438
2019-01-18 14:42:15,639 12 loss=0.11701054509800347
2019-01-18 14:42:15,640 13 loss=0.10671841720872054
2019-01-18 14:42:15,640 14 loss=0.0976877043670195
2019-01-18 14:42:15,641 15 loss=0.08969231008997942
2019-01-18 14:42:15,641 16 loss=0.08256259749787896
2019-01-18 14:42:15,641 17 loss=0.07617016953820385
2019-01-18 14:42:15,642 18 loss=0.07041592977819915
2019-01-18 14:42:15,642 19 loss=0.06522126680556056
2019-01-18 14:42:15,643 20 loss=0.06052188992322793
2019-01-18 14:42:15,643 21 loss=0.05626371121901687
2019-01-18 14:42:15,644 22 loss=0.05240018502050347
2019-01-18 14:42:15,644 23 loss=0.048890615273155795
2019-01-18 14:42:15,645 24 loss=0.0456990675224399
2019-01-18 14:42:15,645 25 loss=0.04279363913335318
2019-01-18 14:42:15,646 26 loss=0.040145933358134794
2019-01-18 14:42:15,646 27 loss=0.03773064743805046
2019-01-18 14:42:15,647 28 loss=0.03552522630940029
2019-01-18 14:42:15,647 29 loss=0.0335095578874942
2019-01-18 14:42:15,648 30 loss=0.031665699093948306
2019-01-18 14:42:15,648 31 loss=0.02997762819919546
2019-01-18 14:42:15,648 32 loss=0.028431021691489096
2019-01-18 14:42:15,649 33 loss=0.02701305463507459
2019-01-18 14:42:15,649 34 loss=0.0257122234210236
2019-01-18 14:42:15,650 35 loss=0.02451818950873436
2019-01-18 14:42:15,650 36 loss=0.02342164246521123
2019-01-18 14:42:15,651 37 loss=0.022414180428287925
2019-01-18 14:42:15,651 38 loss=0.02148820606240985
2019-01-18 14:42:15,652 39 loss=0.02063683612034604
2019-01-18 14:42:15,652 40 loss=0.01985382284029761
2019-01-18 14:42:15,653 41 loss=0.019133485566012583
2019-01-18 14:42:15,653 42 loss=0.018470651154405954
2019-01-18 14:42:15,654 43 loss=0.017860601914270285
2019-01-18 14:42:15,654 44 loss=0.017299029990325187
2019-01-18 14:42:15,654 45 loss=0.016781997263065934
2019-01-18 14:42:15,655 46 loss=0.016305899973881172
2019-01-18 14:42:15,655 47 loss=0.015867437406126762
2019-01-18 14:42:15,656 48 loss=0.015463584056994733
2019-01-18 14:42:15,656 49 loss=0.015091564823548351
2019-01-18 14:42:15,657 50 loss=0.014748832800968545
2019-01-18 14:42:15,657 final loss=0.014748832800968545
2019-01-18 14:42:15,657 best loss=10000000000.0, distortion=10000000000.0, map=0.0, wc_dist=10000000000.0
2019-01-18 14:42:15,657 	 Computing Major Stats lazily... 
2019-01-18 14:42:15,657 		 Completed edges=42 good=42 bad=0
2019-01-18 14:42:15,659 Distortion avg=0.10670193061536211 wc=1.4428687770811621 me=1.1729075745656345 mc=1.2301640882620295 nan_elements=0
2019-01-18 14:42:15,659 MAP = 1.0
2019-01-18 14:42:15,659 scale=[array([1.])]
2019-01-18 15:15:24,289 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/1300.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/1300.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '--checkpoint-freq', '10', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 15:15:24,291 No Model Save selected!
