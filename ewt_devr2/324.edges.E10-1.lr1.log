2019-01-18 14:40:38,347 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/324.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/324.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 14:40:38,347 No Model Save selected!
2019-01-18 14:40:38,348 Loaded Graph data/edges/parsing/ewt/ewt_dev/324.edges with 12 nodes scale=1.0
2019-01-18 14:40:38,348 Building dataset
2019-01-18 14:40:38,349 Subsample: 12 points with scale 1.0 subsample=11
2019-01-18 14:40:38,359 Built Data Sampler
2019-01-18 14:40:38,359 Creating a fresh model warm_start?=None
2019-01-18 14:40:38,359 	 Warmstarting? None None 12
2019-01-18 14:40:38,360 Embedding() torch.Size([12, 10])
2019-01-18 14:40:38,360 relative No Rescale
2019-01-18 14:40:38,360 Constructed model with dim=0 and epochs=0 isnan=False
2019-01-18 14:40:38,360 SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 1
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 2
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 3
    dampening: 0
    initial_lr: 0.0001
    lr: 0.001
    momentum: 0.0
    nesterov: False
    weight_decay: 0
)
2019-01-18 14:40:38,360 *** Initial Checkpoint. Computing Stats
2019-01-18 14:40:38,360 	 Computing Major Stats lazily... 
2019-01-18 14:40:38,361 		 Completed edges=132 good=132 bad=0
2019-01-18 14:40:38,364 Distortion avg=0.9978559373243174 wc=7.4997665091346 me=0.005628252251848828 mc=1332.521389152556 nan_elements=0
2019-01-18 14:40:38,364 MAP = 0.2111471861471861
2019-01-18 14:40:38,364 scale=[array([1.])]
2019-01-18 14:40:38,364 *** End Initial Checkpoint

2019-01-18 14:40:38,422 1 loss=0.9957179958210873
2019-01-18 14:40:38,422 2 loss=0.821863142086039
2019-01-18 14:40:38,423 3 loss=0.6918975277995637
2019-01-18 14:40:38,423 4 loss=0.5969092938119324
2019-01-18 14:40:38,424 5 loss=0.5257755249083118
2019-01-18 14:40:38,425 6 loss=0.4709827802664498
2019-01-18 14:40:38,425 7 loss=0.4276119111030237
2019-01-18 14:40:38,426 8 loss=0.3924299108598648
2019-01-18 14:40:38,426 9 loss=0.3632810754459013
2019-01-18 14:40:38,427 10 loss=0.33869923340567376
2019-01-18 14:40:38,427 11 loss=0.31766380062157396
2019-01-18 14:40:38,428 12 loss=0.2994462317321458
2019-01-18 14:40:38,428 13 loss=0.2835127305930568
2019-01-18 14:40:38,429 14 loss=0.26946197429290747
2019-01-18 14:40:38,429 15 loss=0.2569847388801583
2019-01-18 14:40:38,430 16 loss=0.24583733646261224
2019-01-18 14:40:38,431 17 loss=0.23582385038160214
2019-01-18 14:40:38,431 18 loss=0.22678403992712914
2019-01-18 14:40:38,432 19 loss=0.21858494486596652
2019-01-18 14:40:38,432 20 loss=0.21111493713145144
2019-01-18 14:40:38,433 21 loss=0.20427941443586767
2019-01-18 14:40:38,433 22 loss=0.19799761236924934
2019-01-18 14:40:38,434 23 loss=0.1922001908427512
2019-01-18 14:40:38,435 24 loss=0.18682736599538122
2019-01-18 14:40:38,435 25 loss=0.18182743354525227
2019-01-18 14:40:38,436 26 loss=0.17715557867956813
2019-01-18 14:40:38,436 27 loss=0.17277290011030286
2019-01-18 14:40:38,437 28 loss=0.16864559767333845
2019-01-18 14:40:38,437 29 loss=0.1647442875229725
2019-01-18 14:40:38,438 30 loss=0.161043418963066
2019-01-18 14:40:38,438 31 loss=0.1575207738197879
2019-01-18 14:40:38,439 32 loss=0.15415703402431633
2019-01-18 14:40:38,440 33 loss=0.15093540641690573
2019-01-18 14:40:38,440 34 loss=0.14784129616047592
2019-01-18 14:40:38,441 35 loss=0.1448620218674135
2019-01-18 14:40:38,441 36 loss=0.1419865668036182
2019-01-18 14:40:38,442 37 loss=0.13920536147865228
2019-01-18 14:40:38,442 38 loss=0.1365100936551285
2019-01-18 14:40:38,443 39 loss=0.13389354237892953
2019-01-18 14:40:38,443 40 loss=0.1313494330886818
2019-01-18 14:40:38,444 41 loss=0.12887231123834314
2019-01-18 14:40:38,444 42 loss=0.12645743218157218
2019-01-18 14:40:38,445 43 loss=0.12410066533504849
2019-01-18 14:40:38,446 44 loss=0.1217984108700904
2019-01-18 14:40:38,446 45 loss=0.11954752738481733
2019-01-18 14:40:38,447 46 loss=0.11734526918779888
2019-01-18 14:40:38,447 47 loss=0.11518923198237259
2019-01-18 14:40:38,448 48 loss=0.11307730588144917
2019-01-18 14:40:38,448 49 loss=0.1110076348078997
2019-01-18 14:40:38,449 50 loss=0.1089785814473477
2019-01-18 14:40:38,449 final loss=0.1089785814473477
2019-01-18 14:40:38,449 best loss=10000000000.0, distortion=10000000000.0, map=0.0, wc_dist=10000000000.0
2019-01-18 14:40:38,449 	 Computing Major Stats lazily... 
2019-01-18 14:40:38,450 		 Completed edges=132 good=132 bad=0
2019-01-18 14:40:38,452 Distortion avg=0.2843329441523171 wc=3.7587331864812397 me=1.4916718433588463 mc=2.51981238582447 nan_elements=0
2019-01-18 14:40:38,452 MAP = 0.9559027777777778
2019-01-18 14:40:38,452 scale=[array([1.])]
2019-01-18 15:24:08,931 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/324.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/324.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '--checkpoint-freq', '10', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 15:24:08,933 No Model Save selected!
