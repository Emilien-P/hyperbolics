2019-01-18 14:40:24,337 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/83.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/83.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 14:40:24,337 No Model Save selected!
2019-01-18 14:40:24,339 Loaded Graph data/edges/parsing/ewt/ewt_dev/83.edges with 31 nodes scale=1.0
2019-01-18 14:40:24,339 Building dataset
2019-01-18 14:40:24,340 Subsample: 31 points with scale 1.0 subsample=30
2019-01-18 14:40:24,383 Built Data Sampler
2019-01-18 14:40:24,383 Creating a fresh model warm_start?=None
2019-01-18 14:40:24,383 	 Warmstarting? None None 31
2019-01-18 14:40:24,384 Embedding() torch.Size([31, 10])
2019-01-18 14:40:24,384 relative No Rescale
2019-01-18 14:40:24,384 Constructed model with dim=0 and epochs=0 isnan=False
2019-01-18 14:40:24,384 SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 1
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 2
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 3
    dampening: 0
    initial_lr: 0.0001
    lr: 0.001
    momentum: 0.0
    nesterov: False
    weight_decay: 0
)
2019-01-18 14:40:24,385 *** Initial Checkpoint. Computing Stats
2019-01-18 14:40:24,385 	 Computing Major Stats lazily... 
2019-01-18 14:40:24,389 		 Completed edges=930 good=930 bad=0
2019-01-18 14:40:24,394 Distortion avg=0.998624076241407 wc=14.949628506223146 me=0.005867304364748557 mc=2547.9551727437633 nan_elements=0
2019-01-18 14:40:24,394 MAP = 0.20051677119268646
2019-01-18 14:40:24,394 scale=[array([1.])]
2019-01-18 14:40:24,395 *** End Initial Checkpoint

2019-01-18 14:40:24,454 1 loss=0.9972509531842104
2019-01-18 14:40:24,455 2 loss=0.9694552200530402
2019-01-18 14:40:24,456 3 loss=0.9420248088999161
2019-01-18 14:40:24,457 4 loss=0.9158073813217971
2019-01-18 14:40:24,458 5 loss=0.8908721912962612
2019-01-18 14:40:24,460 6 loss=0.8671938451740044
2019-01-18 14:40:24,461 7 loss=0.8447210185506256
2019-01-18 14:40:24,462 8 loss=0.8233940438902603
2019-01-18 14:40:24,463 9 loss=0.8031512720087239
2019-01-18 14:40:24,464 10 loss=0.7839318024849322
2019-01-18 14:40:24,465 11 loss=0.7656767553982275
2019-01-18 14:40:24,466 12 loss=0.7483298705034859
2019-01-18 14:40:24,467 13 loss=0.7318377664473957
2019-01-18 14:40:24,468 14 loss=0.7161500173878894
2019-01-18 14:40:24,469 15 loss=0.7012191279237532
2019-01-18 14:40:24,471 16 loss=0.6870004506425137
2019-01-18 14:40:24,472 17 loss=0.6734520717450156
2019-01-18 14:40:24,473 18 loss=0.6605346799082011
2019-01-18 14:40:24,474 19 loss=0.648211427646382
2019-01-18 14:40:24,475 20 loss=0.6364477909138616
2019-01-18 14:40:24,476 21 loss=0.625211430526827
2019-01-18 14:40:24,477 22 loss=0.6144720576150619
2019-01-18 14:40:24,478 23 loss=0.6042013044326716
2019-01-18 14:40:24,479 24 loss=0.5943726012803102
2019-01-18 14:40:24,481 25 loss=0.5849610599103271
2019-01-18 14:40:24,482 26 loss=0.5759433635334164
2019-01-18 14:40:24,483 27 loss=0.5672976633779864
2019-01-18 14:40:24,484 28 loss=0.5590034816439248
2019-01-18 14:40:24,485 29 loss=0.5510416206224111
2019-01-18 14:40:24,486 30 loss=0.5433940777106263
2019-01-18 14:40:24,487 31 loss=0.5360439660262684
2019-01-18 14:40:24,488 32 loss=0.5289754403158973
2019-01-18 14:40:24,489 33 loss=0.522173627849277
2019-01-18 14:40:24,490 34 loss=0.5156245639961782
2019-01-18 14:40:24,491 35 loss=0.5093151321905839
2019-01-18 14:40:24,492 36 loss=0.5032330079983975
2019-01-18 14:40:24,493 37 loss=0.49736660701761043
2019-01-18 14:40:24,494 38 loss=0.49170503635368246
2019-01-18 14:40:24,496 39 loss=0.48623804942709925
2019-01-18 14:40:24,497 40 loss=0.48095600388433324
2019-01-18 14:40:24,498 41 loss=0.47584982239747314
2019-01-18 14:40:24,499 42 loss=0.4709109561514454
2019-01-18 14:40:24,500 43 loss=0.4661313508308773
2019-01-18 14:40:24,501 44 loss=0.46150341493120584
2019-01-18 14:40:24,502 45 loss=0.4570199902305415
2019-01-18 14:40:24,503 46 loss=0.4526743242700598
2019-01-18 14:40:24,504 47 loss=0.44846004470129136
2019-01-18 14:40:24,505 48 loss=0.44437113536862993
2019-01-18 14:40:24,506 49 loss=0.4404019140047011
2019-01-18 14:40:24,507 50 loss=0.4365470114249379
2019-01-18 14:40:24,508 final loss=0.4365470114249379
2019-01-18 14:40:24,508 best loss=10000000000.0, distortion=10000000000.0, map=0.0, wc_dist=10000000000.0
2019-01-18 14:40:24,508 	 Computing Major Stats lazily... 
2019-01-18 14:40:24,511 		 Completed edges=930 good=930 bad=0
2019-01-18 14:40:24,516 Distortion avg=0.6305013896800864 wc=17.585654874634088 me=1.7801801506956323 mc=9.878581596229026 nan_elements=0
2019-01-18 14:40:24,516 MAP = 0.07415400454572879
2019-01-18 14:40:24,517 scale=[array([1.])]
2019-01-18 15:20:09,950 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/83.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/83.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '--checkpoint-freq', '10', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 15:20:09,951 No Model Save selected!
