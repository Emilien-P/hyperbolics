2019-01-18 14:40:17,827 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/965.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/965.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 14:40:17,828 No Model Save selected!
2019-01-18 14:40:17,829 Loaded Graph data/edges/parsing/ewt/ewt_dev/965.edges with 27 nodes scale=1.0
2019-01-18 14:40:17,830 Building dataset
2019-01-18 14:40:17,830 Subsample: 27 points with scale 1.0 subsample=26
2019-01-18 14:40:17,864 Built Data Sampler
2019-01-18 14:40:17,864 Creating a fresh model warm_start?=None
2019-01-18 14:40:17,864 	 Warmstarting? None None 27
2019-01-18 14:40:17,865 Embedding() torch.Size([27, 10])
2019-01-18 14:40:17,865 relative No Rescale
2019-01-18 14:40:17,865 Constructed model with dim=0 and epochs=0 isnan=False
2019-01-18 14:40:17,865 SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 1
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 2
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 3
    dampening: 0
    initial_lr: 0.0001
    lr: 0.001
    momentum: 0.0
    nesterov: False
    weight_decay: 0
)
2019-01-18 14:40:17,865 *** Initial Checkpoint. Computing Stats
2019-01-18 14:40:17,865 	 Computing Major Stats lazily... 
2019-01-18 14:40:17,869 		 Completed edges=702 good=702 bad=0
2019-01-18 14:40:17,873 Distortion avg=0.9985537081777865 wc=21.280931977081234 me=0.006841983940850235 mc=3110.3452099650367 nan_elements=0
2019-01-18 14:40:17,873 MAP = 0.21315199512774521
2019-01-18 14:40:17,873 scale=[array([1.])]
2019-01-18 14:40:17,873 *** End Initial Checkpoint

2019-01-18 14:40:17,930 1 loss=0.9971106887619653
2019-01-18 14:40:17,932 2 loss=0.9631877663833783
2019-01-18 14:40:17,934 3 loss=0.9302940583316293
2019-01-18 14:40:17,935 4 loss=0.8992882098569515
2019-01-18 14:40:17,935 5 loss=0.8701899052230027
2019-01-18 14:40:17,936 6 loss=0.8429167640243586
2019-01-18 14:40:17,937 7 loss=0.8173617557476973
2019-01-18 14:40:17,938 8 loss=0.7934130700123445
2019-01-18 14:40:17,939 9 loss=0.7709610850715805
2019-01-18 14:40:17,940 10 loss=0.7499011399191825
2019-01-18 14:40:17,941 11 loss=0.7301346301759774
2019-01-18 14:40:17,942 12 loss=0.7115693459212832
2019-01-18 14:40:17,943 13 loss=0.6941194408671222
2019-01-18 14:40:17,944 14 loss=0.677705216583553
2019-01-18 14:40:17,945 15 loss=0.662252815313699
2019-01-18 14:40:17,946 16 loss=0.6476938716387486
2019-01-18 14:40:17,947 17 loss=0.6339651509665767
2019-01-18 14:40:17,948 18 loss=0.6210081906897219
2019-01-18 14:40:17,949 19 loss=0.6087689529680578
2019-01-18 14:40:17,950 20 loss=0.5971974940459301
2019-01-18 14:40:17,951 21 loss=0.5862476525814496
2019-01-18 14:40:17,952 22 loss=0.5758767579819706
2019-01-18 14:40:17,952 23 loss=0.5660453588303721
2019-01-18 14:40:17,953 24 loss=0.5567169709333436
2019-01-18 14:40:17,954 25 loss=0.547857844194316
2019-01-18 14:40:17,955 26 loss=0.5394367473290539
2019-01-18 14:40:17,956 27 loss=0.53142476935011
2019-01-18 14:40:17,957 28 loss=0.5237951367145388
2019-01-18 14:40:17,958 29 loss=0.5165230450359948
2019-01-18 14:40:17,959 30 loss=0.5095855042934136
2019-01-18 14:40:17,960 31 loss=0.5029611965144563
2019-01-18 14:40:17,961 32 loss=0.4966303449663842
2019-01-18 14:40:17,962 33 loss=0.49057459394583985
2019-01-18 14:40:17,963 34 loss=0.48477689831919263
2019-01-18 14:40:17,963 35 loss=0.4792214220248708
2019-01-18 14:40:17,964 36 loss=0.473893444807122
2019-01-18 14:40:17,965 37 loss=0.4687792765062283
2019-01-18 14:40:17,966 38 loss=0.46386617828285065
2019-01-18 14:40:17,967 39 loss=0.4591422902036541
2019-01-18 14:40:17,968 40 loss=0.4545965646616075
2019-01-18 14:40:17,969 41 loss=0.45021870514734286
2019-01-18 14:40:17,970 42 loss=0.44599910992781394
2019-01-18 14:40:17,971 43 loss=0.44192882022529534
2019-01-18 14:40:17,972 44 loss=0.4379994725237245
2019-01-18 14:40:17,973 45 loss=0.43420325466060034
2019-01-18 14:40:17,974 46 loss=0.43053286539137803
2019-01-18 14:40:17,975 47 loss=0.4269814771396244
2019-01-18 14:40:17,976 48 loss=0.4235427016703636
2019-01-18 14:40:17,977 49 loss=0.42021055844617905
2019-01-18 14:40:17,977 50 loss=0.41697944544590576
2019-01-18 14:40:17,978 final loss=0.41697944544590576
2019-01-18 14:40:17,978 best loss=10000000000.0, distortion=10000000000.0, map=0.0, wc_dist=10000000000.0
2019-01-18 14:40:17,978 	 Computing Major Stats lazily... 
2019-01-18 14:40:17,981 		 Completed edges=702 good=702 bad=0
2019-01-18 14:40:17,985 Distortion avg=0.612088268745288 wc=20.064620758206186 me=1.8965145767082514 mc=10.57973453229767 nan_elements=0
2019-01-18 14:40:17,985 MAP = 0.08692083872363358
2019-01-18 14:40:17,985 scale=[array([1.])]
2019-01-18 15:23:34,150 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/965.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/965.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '--checkpoint-freq', '10', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 15:23:34,151 No Model Save selected!
