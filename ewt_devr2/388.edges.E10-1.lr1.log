2019-01-18 14:40:47,208 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/388.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/388.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 14:40:47,208 No Model Save selected!
2019-01-18 14:40:47,209 Loaded Graph data/edges/parsing/ewt/ewt_dev/388.edges with 8 nodes scale=1.0
2019-01-18 14:40:47,209 Building dataset
2019-01-18 14:40:47,210 Subsample: 8 points with scale 1.0 subsample=7
2019-01-18 14:40:47,216 Built Data Sampler
2019-01-18 14:40:47,216 Creating a fresh model warm_start?=None
2019-01-18 14:40:47,216 	 Warmstarting? None None 8
2019-01-18 14:40:47,217 Embedding() torch.Size([8, 10])
2019-01-18 14:40:47,217 relative No Rescale
2019-01-18 14:40:47,217 Constructed model with dim=0 and epochs=0 isnan=False
2019-01-18 14:40:47,217 SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 1
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 2
    dampening: 0
    initial_lr: 0.1
    lr: 1.0
    momentum: 0.0
    nesterov: False
    weight_decay: 0

Parameter Group 3
    dampening: 0
    initial_lr: 0.0001
    lr: 0.001
    momentum: 0.0
    nesterov: False
    weight_decay: 0
)
2019-01-18 14:40:47,217 *** Initial Checkpoint. Computing Stats
2019-01-18 14:40:47,218 	 Computing Major Stats lazily... 
2019-01-18 14:40:47,218 		 Completed edges=56 good=56 bad=0
2019-01-18 14:40:47,220 Distortion avg=0.9975208888507755 wc=6.817929150513145 me=0.005941110731069004 mc=1147.5849313595224 nan_elements=0
2019-01-18 14:40:47,220 MAP = 0.48035714285714287
2019-01-18 14:40:47,220 scale=[array([1.])]
2019-01-18 14:40:47,220 *** End Initial Checkpoint

2019-01-18 14:40:47,277 1 loss=0.9950499931285606
2019-01-18 14:40:47,278 2 loss=0.7008157614922762
2019-01-18 14:40:47,279 3 loss=0.5328962730688885
2019-01-18 14:40:47,279 4 loss=0.44105898924378645
2019-01-18 14:40:47,280 5 loss=0.3869639673833218
2019-01-18 14:40:47,280 6 loss=0.3518574650070607
2019-01-18 14:40:47,281 7 loss=0.32677025038410307
2019-01-18 14:40:47,281 8 loss=0.3073403165089426
2019-01-18 14:40:47,282 9 loss=0.29136885545848346
2019-01-18 14:40:47,282 10 loss=0.27768612464439796
2019-01-18 14:40:47,283 11 loss=0.26562351667232986
2019-01-18 14:40:47,283 12 loss=0.2547655241620735
2019-01-18 14:40:47,284 13 loss=0.2448318839166903
2019-01-18 14:40:47,284 14 loss=0.23562077046023908
2019-01-18 14:40:47,285 15 loss=0.2269807236032931
2019-01-18 14:40:47,285 16 loss=0.21879603054552174
2019-01-18 14:40:47,286 17 loss=0.21097831407547232
2019-01-18 14:40:47,286 18 loss=0.20346093792307215
2019-01-18 14:40:47,287 19 loss=0.1961947178559791
2019-01-18 14:40:47,287 20 loss=0.1891443325694604
2019-01-18 14:40:47,288 21 loss=0.18228524152724848
2019-01-18 14:40:47,288 22 loss=0.175601076925348
2019-01-18 14:40:47,289 23 loss=0.169081511450306
2019-01-18 14:40:47,289 24 loss=0.16272058454663388
2019-01-18 14:40:47,289 25 loss=0.15651543818962552
2019-01-18 14:40:47,290 26 loss=0.15046538924986017
2019-01-18 14:40:47,291 27 loss=0.1445712566355987
2019-01-18 14:40:47,291 28 loss=0.13883486659261787
2019-01-18 14:40:47,291 29 loss=0.13325867405606184
2019-01-18 14:40:47,292 30 loss=0.12784545623863777
2019-01-18 14:40:47,292 31 loss=0.12259805218317703
2019-01-18 14:40:47,293 32 loss=0.11751913601631835
2019-01-18 14:40:47,293 33 loss=0.11261102101760156
2019-01-18 14:40:47,294 34 loss=0.10787549653900316
2019-01-18 14:40:47,294 35 loss=0.10331370119205772
2019-01-18 14:40:47,295 36 loss=0.09892603476030466
2019-01-18 14:40:47,295 37 loss=0.09471210914020749
2019-01-18 14:40:47,296 38 loss=0.09067073617406521
2019-01-18 14:40:47,296 39 loss=0.08679994812152426
2019-01-18 14:40:47,297 40 loss=0.08309704504249223
2019-01-18 14:40:47,297 41 loss=0.07955866262578655
2019-01-18 14:40:47,298 42 loss=0.07618085393290462
2019-01-18 14:40:47,298 43 loss=0.07295917898864325
2019-01-18 14:40:47,299 44 loss=0.06988879696437646
2019-01-18 14:40:47,299 45 loss=0.06696455669815093
2019-01-18 14:40:47,300 46 loss=0.06418108234007665
2019-01-18 14:40:47,300 47 loss=0.06153285190087371
2019-01-18 14:40:47,301 48 loss=0.05901426735199526
2019-01-18 14:40:47,301 49 loss=0.056619715645181604
2019-01-18 14:40:47,302 50 loss=0.05434362057947518
2019-01-18 14:40:47,302 final loss=0.05434362057947518
2019-01-18 14:40:47,302 best loss=10000000000.0, distortion=10000000000.0, map=0.0, wc_dist=10000000000.0
2019-01-18 14:40:47,302 	 Computing Major Stats lazily... 
2019-01-18 14:40:47,302 		 Completed edges=56 good=56 bad=0
2019-01-18 14:40:47,304 Distortion avg=0.1875755624527286 wc=2.2241348795619187 me=1.1986710712781867 mc=1.8555005896573797 nan_elements=0
2019-01-18 14:40:47,304 MAP = 1.0
2019-01-18 14:40:47,304 scale=[array([1.])]
2019-01-18 15:20:56,012 Commandline ['pytorch/pytorch_hyperbolic.py', 'learn', 'data/edges/parsing/ewt/ewt_dev/388.edges', '--dim', '0', '--hyp', '0', '--edim', '10', '--euc', '1', '--sdim', '0', '--sph', '0', '--log-name', 'ewt_devr2/388.edges.E10-1.lr1.log', '--batch-size', '65536', '--epochs', '50', '--checkpoint-freq', '10', '-g', '--subsample', '1024', '--learning-rate', '1']
2019-01-18 15:20:56,012 No Model Save selected!
